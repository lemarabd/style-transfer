{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20191948\\Anaconda3\\envs\\5aua0\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\20191948\\Anaconda3\\envs\\5aua0\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target,):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # we 'detach' the target content from the tree used\n",
    "        # to dynamically compute the gradient: this is a stated value,\n",
    "        # not a variable. Otherwise the forward method of the criterion\n",
    "        # will throw an error.\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "######################################################################\n",
    "# .. Note::\n",
    "#    **Important detail**: although this module is named ``ContentLoss``, it\n",
    "#    is not a true PyTorch Loss function. If you want to define your content\n",
    "#    loss as a PyTorch Loss function, you have to create a PyTorch autograd function \n",
    "#    to recompute/implement the gradient manually in the ``backward``\n",
    "#    method.\n",
    "\n",
    "######################################################################\n",
    "# Style Loss\n",
    "# ~~~~~~~~~~\n",
    "# \n",
    "# The style loss module is implemented similarly to the content loss\n",
    "# module. It will act as a transparent layer in a\n",
    "# network that computes the style loss of that layer. In order to\n",
    "# calculate the style loss, we need to compute the gram matrix :math:`G_{XL}`. A gram\n",
    "# matrix is the result of multiplying a given matrix by its transposed\n",
    "# matrix. In this application the given matrix is a reshaped version of\n",
    "# the feature maps :math:`F_{XL}` of a layer :math:`L`. :math:`F_{XL}` is reshaped to form :math:`\\hat{F}_{XL}`, a :math:`K`\\ x\\ :math:`N`\n",
    "# matrix, where :math:`K` is the number of feature maps at layer :math:`L` and :math:`N` is the\n",
    "# length of any vectorized feature map :math:`F_{XL}^k`. For example, the first line\n",
    "# of :math:`\\hat{F}_{XL}` corresponds to the first vectorized feature map :math:`F_{XL}^1`.\n",
    "# \n",
    "# Finally, the gram matrix must be normalized by dividing each element by\n",
    "# the total number of elements in the matrix. This normalization is to\n",
    "# counteract the fact that :math:`\\hat{F}_{XL}` matrices with a large :math:`N` dimension yield\n",
    "# larger values in the Gram matrix. These larger values will cause the\n",
    "# first layers (before pooling layers) to have a larger impact during the\n",
    "# gradient descent. Style features tend to be in the deeper layers of the\n",
    "# network so this normalization step is crucial.\n",
    "# \n",
    "\n",
    "def gram_matrix(input):\n",
    "    a, b, c, d = input.size()  # a=batch size(=1)\n",
    "    # b=number of feature maps\n",
    "    # (c,d)=dimensions of a f. map (N=c*d)\n",
    "\n",
    "    features = input.view(a * b, c * d)  # resize F_XL into \\hat F_XL\n",
    "\n",
    "    G = torch.mm(features, features.t())  # compute the gram product\n",
    "\n",
    "    # we 'normalize' the values of the gram matrix\n",
    "    # by dividing by the number of element in each feature maps.\n",
    "    return G.div(a * b * c * d)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Now the style loss module looks almost exactly like the content loss\n",
    "# module. The style distance is also computed using the mean square\n",
    "# error between :math:`G_{XL}` and :math:`G_{SL}`.\n",
    "# \n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Importing the Model\n",
    "# -------------------\n",
    "# \n",
    "# Now we need to import a pretrained neural network. We will use a 19\n",
    "# layer VGG network like the one used in the paper.\n",
    "# \n",
    "# PyTorch’s implementation of VGG is a module divided into two child\n",
    "# ``Sequential`` modules: ``features`` (containing convolution and pooling layers),\n",
    "# and ``classifier`` (containing fully connected layers). We will use the\n",
    "# ``features`` module because we need the output of the individual\n",
    "# convolution layers to measure content and style loss. Some layers have\n",
    "# different behavior during training than evaluation, so we must set the\n",
    "# network to evaluation mode using ``.eval()``.\n",
    "# \n",
    "\n",
    "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Additionally, VGG networks are trained on images with each channel\n",
    "# normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n",
    "# We will use them to normalize the image before sending it into the network.\n",
    "# \n",
    "\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "# create a module to normalize input image so we can easily put it in a\n",
    "# ``nn.Sequential``\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        # .view the mean and std to make them [C x 1 x 1] so that they can\n",
    "        # directly work with image Tensor of shape [B x C x H x W].\n",
    "        # B is batch size. C is number of channels. H is height and W is width.\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # normalize ``img``\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# A ``Sequential`` module contains an ordered list of child modules. For\n",
    "# instance, ``vgg19.features`` contains a sequence (``Conv2d``, ``ReLU``, ``MaxPool2d``,\n",
    "# ``Conv2d``, ``ReLU``…) aligned in the right order of depth. We need to add our\n",
    "# content loss and style loss layers immediately after the convolution\n",
    "# layer they are detecting. To do this we must create a new ``Sequential``\n",
    "# module that has content loss and style loss modules correctly inserted.\n",
    "# \n",
    "\n",
    "# desired depth layers to compute style/content losses :\n",
    "content_layers_default = ['conv_4']\n",
    "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
    "                               style_img, content_img,\n",
    "                               content_layers=content_layers_default,\n",
    "                               style_layers=style_layers_default):\n",
    "    # normalization module\n",
    "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "\n",
    "    # just in order to have an iterable access to or list of content/style\n",
    "    # losses\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    # assuming that ``cnn`` is a ``nn.Sequential``, so we make a new ``nn.Sequential``\n",
    "    # to put in modules that are supposed to be activated sequentially\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    i = 0  # increment every time we see a conv\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            # The in-place version doesn't play very nicely with the ``ContentLoss``\n",
    "            # and ``StyleLoss`` we insert below. So we replace with out-of-place\n",
    "            # ones here.\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'bn_{}'.format(i)\n",
    "        else:\n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            # add content loss:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            # add style loss:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # now we trim off the layers after the last content and style losses\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "\n",
    "    model = model[:(i + 1)]\n",
    "\n",
    "    return model, style_losses, content_losses\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Gradient Descent\n",
    "# ----------------\n",
    "# \n",
    "# As Leon Gatys, the author of the algorithm, suggested `here <https://discuss.pytorch.org/t/pytorch-tutorial-for-neural-transfert-of-artistic-style/336/20?u=alexis-jacq>`__, we will use\n",
    "# L-BFGS algorithm to run our gradient descent. Unlike training a network,\n",
    "# we want to train the input image in order to minimize the content/style\n",
    "# losses. We will create a PyTorch L-BFGS optimizer ``optim.LBFGS`` and pass\n",
    "# our image to it as the tensor to optimize.\n",
    "# \n",
    "\n",
    "def get_input_optimizer(input_img):\n",
    "    # this line to show that input is a parameter that requires a gradient\n",
    "    optimizer = optim.LBFGS([input_img])\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Finally, we must define a function that performs the neural transfer. For\n",
    "# each iteration of the networks, it is fed an updated input and computes\n",
    "# new losses. We will run the ``backward`` methods of each loss module to\n",
    "# dynamically compute their gradients. The optimizer requires a “closure”\n",
    "# function, which reevaluates the module and returns the loss.\n",
    "# \n",
    "# We still have one final constraint to address. The network may try to\n",
    "# optimize the input with values that exceed the 0 to 1 tensor range for\n",
    "# the image. We can address this by correcting the input values to be\n",
    "# between 0 to 1 each time the network is run.\n",
    "# \n",
    "\n",
    "def run_style_transfer(cnn, normalization_mean, normalization_std,\n",
    "                       content_img, style_img, input_img, num_steps=300,\n",
    "                       style_weight=1000000, content_weight=1000):\n",
    "    \"\"\"Run the style transfer.\"\"\"\n",
    "    print('Building the style transfer model..')\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
    "        normalization_mean, normalization_std, style_img, content_img)\n",
    "\n",
    "    # We want to optimize the input and not the model parameters so we\n",
    "    # update all the requires_grad fields accordingly\n",
    "\n",
    "    # we also only want to style_transfer if the style_loss is higher than 1000\n",
    "    # if style_loss < 1000:\n",
    "    #     return input_img\n",
    "    \n",
    "\n",
    "    input_img.requires_grad_(True)\n",
    "    model.requires_grad_(False)\n",
    "\n",
    "    optimizer = get_input_optimizer(input_img)\n",
    "\n",
    "    print('Optimizing..')\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            # correct the values of updated input image\n",
    "            with torch.no_grad():\n",
    "                input_img.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "\n",
    "            for sl in style_losses:\n",
    "                style_score += sl.loss\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.loss\n",
    "\n",
    "            style_score *= style_weight\n",
    "            content_score *= content_weight\n",
    "\n",
    "            loss = style_score + content_score\n",
    "            loss.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(\"run {}:\".format(run))\n",
    "                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
    "                    style_score.item(), content_score.item()))\n",
    "                print()\n",
    "      \n",
    "            return style_score + content_score\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # a last correction...\n",
    "    with torch.no_grad():\n",
    "        input_img.clamp_(0, 1)\n",
    "\n",
    "    return input_img\n",
    "\n",
    "# function that returns the style_score and content_score for a given style_img\n",
    "def get_style_score(style_img, content_img, cnn, normalization_mean, normalization_std,\n",
    "                    style_weight=1000000, content_weight=1000):\n",
    "    # compare the style img and content img and get the initial loss and return the style score, to see how much the images differ\n",
    "    \n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
    "        normalization_mean, normalization_std, style_img, content_img)\n",
    "    input_img = content_img.clone()\n",
    "    input_img.requires_grad_(True)\n",
    "    model.requires_grad_(False)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_img.clamp_(0, 1)\n",
    "\n",
    "    optimizer = get_input_optimizer(input_img) \n",
    "    optimizer.zero_grad()\n",
    "    model(input_img)\n",
    "    style_score = 0\n",
    "    print(style_losses)\n",
    "    for sl in style_losses:\n",
    "        style_score += sl.loss\n",
    "\n",
    "    style_score *= style_weight\n",
    "\n",
    "    loss = style_score\n",
    "    loss.backward()\n",
    "\n",
    "    return style_score.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired size of the output image\n",
    "imsize = 512 if torch.cuda.is_available() else 128  # use small size if no GPU\n",
    "\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize(imsize),  # scale imported image\n",
    "    transforms.ToTensor()])  # transform it into a torch tensor\n",
    "\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    # fake batch dimension required to fit network's input dimensions\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "# save_image function\n",
    "def save_image(tensor, filename):\n",
    "    # unnormalize\n",
    "    tensor = tensor.cpu().clone()\n",
    "    tensor = tensor.squeeze(0)\n",
    "    tensor = unloader(tensor)\n",
    "    # save image\n",
    "    im = Image.fromarray(tensor)\n",
    "    im.save(filename)\n",
    "\n",
    "# load images\n",
    "def unloader(tensor):\n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    image = transforms.ToPILImage()(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# style_img = image_loader(\"../339039a4-ce3be003.jpg\")\n",
    "# content_img = image_loader(\"../aachen_000001_000019_leftImg8bit.png\")\n",
    "\n",
    "# # resize the larger image to the smaller one\n",
    "# if style_img.size() < content_img.size():\n",
    "#     content_img = transforms.Resize(style_img.size()[2:])(content_img)\n",
    "# elif style_img.size() > content_img.size():\n",
    "#     style_img = transforms.Resize(content_img.size()[2:])(style_img)\n",
    "\n",
    "# assert style_img.size() == content_img.size(), \\\n",
    "#     \"we need to import style and content images of the same size\"\n",
    "\n",
    "# get_style_score(style_img, content_img, cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "#                     style_weight=1000000, content_weight=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20191948\\AppData\\Local\\Temp\\ipykernel_24736\\1185840379.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
      "C:\\Users\\20191948\\AppData\\Local\\Temp\\ipykernel_24736\\1185840379.py:117: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.std = torch.tensor(std).view(-1, 1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 402.894287 Content Loss: 388.491821\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 322.455017 Content Loss: 351.689728\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 292.315369 Content Loss: 336.862366\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 329.276672 Content Loss: 551.741760\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 281.530670 Content Loss: 344.138214\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 386.537933 Content Loss: 787.857544\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 762.186157 Content Loss: 987.491272\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 662.913452 Content Loss: 815.800903\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 621.253296 Content Loss: 779.747070\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 607.254578 Content Loss: 757.339417\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 597.350647 Content Loss: 850.568665\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 595.392761 Content Loss: 756.701477\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 595.486328 Content Loss: 494.325104\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 422.040771 Content Loss: 382.547272\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 355.099640 Content Loss: 373.937134\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 311.147186 Content Loss: 377.585297\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 296.584503 Content Loss: 388.829102\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 288.515808 Content Loss: 398.073792\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 614.624756 Content Loss: 589.899902\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 548.622192 Content Loss: 604.072815\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 538.599365 Content Loss: 652.410828\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 541.289307 Content Loss: 670.494324\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 798.736206 Content Loss: 1208.768921\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 620.324402 Content Loss: 964.742554\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 912.306030 Content Loss: 730.347839\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 811.348694 Content Loss: 733.594116\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 779.737122 Content Loss: 720.511475\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 757.258240 Content Loss: 721.800293\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 745.398804 Content Loss: 716.282593\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 735.782288 Content Loss: 724.968384\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 704.572266 Content Loss: 679.927185\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 626.031555 Content Loss: 671.890930\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 594.771240 Content Loss: 659.001831\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 591.427612 Content Loss: 645.198242\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 1003.763306 Content Loss: 2334.168213\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 590.786194 Content Loss: 640.608887\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 664.623230 Content Loss: 420.337311\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 635.717285 Content Loss: 425.252747\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 622.219971 Content Loss: 426.011749\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 613.817383 Content Loss: 426.157776\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 607.866699 Content Loss: 426.215149\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 603.253296 Content Loss: 426.466034\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 710.335999 Content Loss: 634.039917\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 452.690887 Content Loss: 463.661835\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 347.771698 Content Loss: 418.737244\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 302.633453 Content Loss: 391.117462\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 289.547607 Content Loss: 376.765747\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 282.252441 Content Loss: 371.126343\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 691.068054 Content Loss: 1102.765625\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 620.032776 Content Loss: 1045.503662\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 661.271973 Content Loss: 1314.728760\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 1117.390381 Content Loss: 2869.802979\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 902.347595 Content Loss: 2387.017334\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 1117.234863 Content Loss: 2880.330078\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 1256.422852 Content Loss: 1148.114014\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 729.677368 Content Loss: 1153.181030\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 678.394592 Content Loss: 1082.305786\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 642.141357 Content Loss: 1054.534302\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 673.370117 Content Loss: 2500.538818\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 632.651550 Content Loss: 1050.312988\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 1102.414185 Content Loss: 985.544250\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 1023.176697 Content Loss: 1010.773071\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 986.259460 Content Loss: 1018.007141\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 954.941711 Content Loss: 1039.273926\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 958.962585 Content Loss: 1014.957764\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 931.243530 Content Loss: 1170.073730\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 554.525391 Content Loss: 530.836792\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 479.073914 Content Loss: 521.047974\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 443.969025 Content Loss: 620.206909\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 430.162018 Content Loss: 537.905640\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 471.409271 Content Loss: 752.657288\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 430.852905 Content Loss: 615.183472\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 599.959290 Content Loss: 542.229065\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 535.539917 Content Loss: 542.329712\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 509.378143 Content Loss: 534.529358\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 498.101532 Content Loss: 528.941956\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 490.454498 Content Loss: 525.862061\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 490.576843 Content Loss: 532.537109\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 1114.097046 Content Loss: 541.698303\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 896.245911 Content Loss: 673.965393\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 837.594971 Content Loss: 675.406433\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 768.145081 Content Loss: 799.706421\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 20007.677734 Content Loss: 20234.357422\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 937.124817 Content Loss: 1212.516724\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 927.621277 Content Loss: 628.267456\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 696.212402 Content Loss: 632.284729\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 645.755981 Content Loss: 630.227234\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 616.019165 Content Loss: 812.702148\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 591.310791 Content Loss: 668.813477\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 773.945679 Content Loss: 1185.823853\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 803.269714 Content Loss: 911.422546\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 682.687622 Content Loss: 885.412964\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 607.658081 Content Loss: 883.231262\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 581.389893 Content Loss: 944.287842\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 564.645813 Content Loss: 1073.952026\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 601.339844 Content Loss: 1436.119507\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 866.832764 Content Loss: 995.565674\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 788.143250 Content Loss: 964.264221\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 3545.215820 Content Loss: 9732.291992\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 1150.185547 Content Loss: 3647.024414\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 1727.918823 Content Loss: 4771.289551\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 11876671.000000 Content Loss: 342792.187500\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 749.934570 Content Loss: 510.147217\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 679.450562 Content Loss: 519.595642\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 630.695862 Content Loss: 525.369446\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 592.816284 Content Loss: 536.232300\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 570.033813 Content Loss: 536.323853\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 550.250244 Content Loss: 542.830566\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 2087.840576 Content Loss: 1550.651001\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 1400.131348 Content Loss: 1507.560425\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 1274.780640 Content Loss: 1428.003662\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 1242.203613 Content Loss: 1388.034180\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 1579.748413 Content Loss: 2557.118164\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 1234.947754 Content Loss: 1345.215454\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 921.616333 Content Loss: 789.249512\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 843.401306 Content Loss: 779.858276\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 792.304871 Content Loss: 804.353027\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 769.595764 Content Loss: 839.454773\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 1040.643066 Content Loss: 1487.854614\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 2624.269287 Content Loss: 5580.265625\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 543.411011 Content Loss: 409.916351\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 508.091248 Content Loss: 404.063263\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 490.908386 Content Loss: 417.730713\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 486.555756 Content Loss: 405.876831\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 484.057922 Content Loss: 435.331573\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 502.893829 Content Loss: 521.964050\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 729.654175 Content Loss: 656.644958\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 666.051880 Content Loss: 647.443420\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 647.618774 Content Loss: 636.102600\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 640.617798 Content Loss: 628.399597\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 642.862732 Content Loss: 647.109375\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 639.519897 Content Loss: 656.773193\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 599.070068 Content Loss: 380.651154\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 523.390015 Content Loss: 396.265564\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 464.929901 Content Loss: 432.635468\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 454.067474 Content Loss: 733.783508\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 401.896240 Content Loss: 568.845947\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 1201.237183 Content Loss: 3475.661621\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 870.985840 Content Loss: 910.864563\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 786.272949 Content Loss: 881.975220\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 752.109802 Content Loss: 871.905945\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 735.081787 Content Loss: 879.473999\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 847.854614 Content Loss: 1253.735474\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 741.808899 Content Loss: 964.709595\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 630.236877 Content Loss: 496.207825\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 552.809753 Content Loss: 502.685120\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 531.929626 Content Loss: 511.204010\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 669.962280 Content Loss: 1597.451538\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 534.612000 Content Loss: 501.629883\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 990.064575 Content Loss: 1987.136841\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 2407.233398 Content Loss: 4006.129639\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 2308.927490 Content Loss: 3841.920410\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 2445.057129 Content Loss: 4601.440430\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 29502332.000000 Content Loss: 505223.937500\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 2859.855469 Content Loss: 6266.360352\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 7986602.000000 Content Loss: 300634.750000\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 848.383118 Content Loss: 676.822876\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 764.553101 Content Loss: 654.116760\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 748.184937 Content Loss: 639.077515\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 749.092346 Content Loss: 823.509277\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 720.846436 Content Loss: 638.544434\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 708.480957 Content Loss: 669.291138\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 768.140808 Content Loss: 631.019531\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 684.449707 Content Loss: 627.465942\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 624.293701 Content Loss: 655.448608\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 610.486206 Content Loss: 875.290039\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 51861.636719 Content Loss: 30039.621094\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 1366.410645 Content Loss: 2498.740723\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 647.807129 Content Loss: 555.546753\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 549.625305 Content Loss: 576.283142\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 510.902191 Content Loss: 582.987610\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 499.109100 Content Loss: 600.471741\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 517.184753 Content Loss: 711.469604\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 526.765320 Content Loss: 744.505615\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 669.309082 Content Loss: 809.121155\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 614.533081 Content Loss: 764.091125\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 591.605286 Content Loss: 738.449829\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 584.177063 Content Loss: 739.364990\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 576.403748 Content Loss: 750.115540\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 577.821289 Content Loss: 832.585449\n",
      "\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "Building the style transfer model..\n",
      "Optimizing..\n"
     ]
    }
   ],
   "source": [
    "style_dir = \"data\"\n",
    "content_dir = \"data/leftImg8bit/train\"\n",
    "\n",
    "# # run style transfer for all images in the content directory and save the results in folder \"results\"\t\n",
    "# for content_img_name in os.listdir(content_dir):\n",
    "#     content_img = image_loader(os.path.join(content_dir, content_img_name))\n",
    "#     for style_img_name in os.listdir(style_dir):\n",
    "#         style_img = image_loader(os.path.join(style_dir, style_img_name))\n",
    "#         output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "#                                     content_img, style_img, content_img)\n",
    "#         output_name = os.path.join(\"results\", content_img_name[:-4] + \"_\" + style_img_name[:-4] + \".jpg\")\n",
    "#         save_image(output, output_name)\n",
    "\n",
    "# # run style transfer for all images in the content dir and style dir\n",
    "# for content_subdir in os.listdir(content_dir):\n",
    "#     for content_img_name in os.listdir(os.path.join(content_dir, content_subdir)):\n",
    "#         content_img = image_loader(os.path.join(content_dir, content_subdir, content_img_name))\n",
    "#         for style_img_name in os.listdir(style_dir):\n",
    "#             style_img = image_loader(os.path.join(style_dir, style_img_name))\n",
    "\n",
    "#             # resize the larger image to the smaller one\n",
    "#             if style_img.size() < content_img.size():\n",
    "#                 content_img = transforms.Resize(style_img.size()[2:])(content_img)\n",
    "#             elif style_img.size() > content_img.size():\n",
    "#                 style_img = transforms.Resize(content_img.size()[2:])(style_img)\n",
    "\n",
    "#             output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "#                                         content_img, style_img, content_img)\n",
    "#             output_name = os.path.join(\"results\", content_subdir, content_img_name[:-4] + \"_\" + style_img_name[:-4] + \".jpg\")\n",
    "\n",
    "#             # create the results subfolder if it does not exist\n",
    "#             if not os.path.exists(os.path.join(\"results\", content_subdir)):\n",
    "#                 os.makedirs(os.path.join(\"results\", content_subdir))\n",
    "#             save_image(output, output_name)\n",
    "\n",
    "for content_subdir in os.listdir(content_dir):\n",
    "    for content_img_name in os.listdir(os.path.join(content_dir, content_subdir)):\n",
    "        content_img = image_loader(os.path.join(content_dir, content_subdir, content_img_name))\n",
    "        style_img_name = random.choice(os.listdir(style_dir))\n",
    "        style_img = image_loader(os.path.join(style_dir, style_img_name))\n",
    "        \n",
    "        # resize the larger image to the smaller one\n",
    "        if style_img.size() < content_img.size():\n",
    "            content_img = transforms.Resize(style_img.size()[2:])(content_img)\n",
    "        elif style_img.size() > content_img.size():\n",
    "            style_img = transforms.Resize(content_img.size()[2:])(style_img)\n",
    "            \n",
    "        # if style score is lower than 1000, then skip this pair of images and choose new style image\n",
    "        style_score = get_style_score(style_img, content_img, cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                    style_weight=1000000, content_weight=1000)\n",
    "        if style_score < 1500:\n",
    "            continue\n",
    "\n",
    "        output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                                    content_img, style_img, content_img)\n",
    "        output_name = os.path.join(\"results\", content_img_name[:-4] + \"_\" + style_img_name[:-4] + \".jpg\")\n",
    "\n",
    "        transform = T.ToPILImage()\n",
    "        output = transform(output.squeeze(0))\n",
    "        output.save(output_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5aua0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
